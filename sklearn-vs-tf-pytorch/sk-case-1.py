# ---- ğŸ“‹ åœºæ™¯ 1ï¼šé™æ€æ£€æµ‹ï¼ˆå¿«ç…§è¯Šæ–­ï¼‰ç‡ƒæ°”æ³„æ¼é¢„æµ‹æ¨¡å‹ ----
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆåŒ…æ‹¬æ¸©åº¦ã€å‹åŠ›ã€æµé‡å’Œæ³„æ¼æ ‡ç­¾ï¼‰
#--------------------------------------------------

np.random.seed(42)  # å›ºå®šéšæœºæ•°ç§å­ï¼Œç¡®ä¿ç»“æœå¯é‡ç°

# æ­£å¸¸å·¥å†µæ•°æ®ï¼ˆæ— æ³„æ¼ï¼‰--- å†³ç­–æ ‘ä¸å…³å¿ƒæ—¶é—´æˆ³
normal_data = {
  "æ¸©åº¦(â„ƒ)": np.random.normal(25, 2, 500),
  "å‹åŠ›(MPa)": np.random.normal(1.2, 0.1, 500),
  "æµé‡(mÂ³/s)": np.random.normal(0.8, 0.05, 500),
  "æ³„æ¼çŠ¶æ€": 0  # 0è¡¨ç¤ºæ— æ³„æ¼
}

# å¼‚å¸¸å·¥å†µæ•°æ®ï¼ˆæ³„æ¼å‘ç”Ÿï¼‰--- å†³ç­–æ ‘ä¸å…³å¿ƒæ—¶é—´æˆ³
leak_data = {
  "æ¸©åº¦(â„ƒ)": np.random.normal(28, 3, 500),
  "å‹åŠ›(MPa)": np.random.normal(0.9, 0.2, 500),
  "æµé‡(mÂ³/s)": np.random.normal(1.5, 0.3, 500),
  "æ³„æ¼çŠ¶æ€": 1  # 1è¡¨ç¤ºæ³„æ¼
}

# - ç®—æ³•ä¼šä»æ‰€æœ‰æ ·æœ¬é‡Œï¼Œé’ˆå¯¹æ¯ä¸ªç‰¹å¾ï¼Œå°è¯•å¤šä¸ªâ€œé˜ˆå€¼â€
#  ï¼ˆæ¯”å¦‚æ¸©åº¦ < 28â„ƒï¼Œå‹åŠ› > 1.15MPaï¼‰ä½œä¸ºåˆ†è£‚ç‚¹ã€‚=> å†³ç­–æ ‘ => éšæœºæ£®æ—
#-----------------------------------------------------------------------
# ç®—æ³•å°è¯•ä»¥ä¸‹åˆ†è£‚ï¼š
#
# ç”¨æ¸©åº¦ < 28â„ƒ åˆ†è£‚ï¼š
#   å·¦è¾¹ï¼ˆæ¸©åº¦ < 28ï¼‰ï¼šæ ·æœ¬ 25â„ƒã€27â„ƒï¼ˆå‡æ— æ³„æ¼ï¼‰
#   å³è¾¹ï¼ˆæ¸©åº¦ â‰¥ 28ï¼‰ï¼šæ ·æœ¬ 29â„ƒã€30â„ƒï¼ˆå‡æœ‰æ³„æ¼ï¼‰
#   çº¯åº¦é«˜ï¼Œåˆ†è£‚æ•ˆæœå¥½ã€‚
#
# ç”¨å‹åŠ› < 1.15MPa åˆ†è£‚ï¼š
#   å·¦è¾¹ï¼ˆå‹åŠ› < 1.15ï¼‰ï¼šæ ·æœ¬ 1.0ã€1.1ï¼ˆå‡æœ‰æ³„æ¼ï¼‰
#   å³è¾¹ï¼ˆå‹åŠ› â‰¥ 1.15ï¼‰ï¼šæ ·æœ¬ 1.2ã€1.3ï¼ˆå‡æ— æ³„æ¼ï¼‰
#   çº¯åº¦ä¹Ÿé«˜ã€‚
#
# ç”¨æµé‡ < 0.7 åˆ†è£‚ï¼š
#   å·¦è¾¹ï¼ˆæµé‡ < 0.7ï¼‰ï¼šæ ·æœ¬ 0.5ã€0.6ï¼ˆå‡æœ‰æ³„æ¼ï¼‰
#   å³è¾¹ï¼ˆæµé‡ â‰¥ 0.7ï¼‰ï¼šæ ·æœ¬ 0.7ã€0.8ï¼ˆå‡æ— æ³„æ¼ï¼‰
#   çº¯åº¦é«˜ã€‚
#
# ç®—æ³•ä¼šè®¡ç®—æ¯ç§åˆ†è£‚çš„â€œçº¯åº¦æŒ‡æ ‡â€ï¼Œé€‰æ‹©å…¶ä¸­æå‡æœ€å¤§çš„ï¼Œ
# æ¯”å¦‚â€œæ¸©åº¦ < 28â„ƒâ€ä½œä¸ºç¬¬ä¸€ä¸ªåˆ¤æ–­æ¡ä»¶ã€‚
#----------------------------------------------
# æœ€ç»ˆå†³ç­–æ ‘å¤§è‡´åƒè¿™æ ·ï¼ˆç®€åŒ–ç‰ˆï¼‰
#
# æ¸©åº¦ < 28â„ƒ
#  â”œâ”€ æ˜¯ï¼šæ— æ³„æ¼
#  â””â”€ å¦ï¼šè¿›ä¸€æ­¥åˆ¤æ–­å‹åŠ›
#       â””â”€ å‹åŠ› â‰¥ 1.15
#          â”œâ”€ æ˜¯ï¼šæ— æ³„æ¼
#          â””â”€ å¦ï¼šæ³„æ¼

# åˆå¹¶æ•°æ®
df_normal = pd.DataFrame(normal_data)
df_leak = pd.DataFrame(leak_data)
df = pd.concat([df_normal, df_leak], ignore_index=True)

# 2. æ•°æ®é¢„å¤„ç†
#--------------------------------------------------

# æ´—ç‰Œæ•°æ®ï¼ˆæ‰“ä¹±é¡ºåºï¼‰
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
X = df[["æ¸©åº¦(â„ƒ)", "å‹åŠ›(MPa)", "æµé‡(mÂ³/s)"]]
y = df["æ³„æ¼çŠ¶æ€"]

# 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
  X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
model = RandomForestClassifier(
  n_estimators=100,  # æ ‘çš„æ•°é‡
  max_depth=5,       # æ ‘çš„æœ€å¤§æ·±åº¦
  random_state=42
)
model.fit(X_train, y_train)

print('-- è®­ç»ƒ OK.')

# 6. æ¨¡å‹è¯„ä¼°
#--------------------------------------------------

# è®­ç»ƒé›†è¯„ä¼°
train_preds = model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_preds)

# æµ‹è¯•é›†è¯„ä¼°
test_preds = model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_preds)

# æ‰“å°è¯„ä¼°ç»“æœ
print("\nğŸ”¥ ç‡ƒæ°”æ³„æ¼æ£€æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š ğŸ”¥")
print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.2%}")
print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.2%}")
print("\næ··æ·†çŸ©é˜µ:")
print(confusion_matrix(y_test, test_preds))
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, test_preds))

# 7. æ¨¡æ‹Ÿå®æ—¶ç›‘æµ‹é¢„æµ‹
print("\nğŸ”” å®æ—¶ç›‘æµ‹é¢„æµ‹æ¼”ç¤º ğŸ””")
# éšæœºç”Ÿæˆä¸€ä¸ªä¼ æ„Ÿå™¨è¯»æ•°æ ·æœ¬
samples = np.array([
  [26.5, 1.15, 0.82],    # æ­£å¸¸æ ·æœ¬1
  [25.8, 0.85, 1.45],    # æ³„æ¼æ ·æœ¬
  [27.2, 1.18, 0.78],    # æ­£å¸¸æ ·æœ¬2
  [30.1, 0.92, 1.35]     # æ³„æ¼æ ·æœ¬
])

# å¯¹æ ·æœ¬è¿›è¡Œç›¸åŒçš„æ ‡å‡†åŒ–å¤„ç†
samples_scaled = scaler.transform(samples)

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
predictions = model.predict(samples_scaled)
leak_probabilities = model.predict_proba(samples_scaled)[:, 1] * 100  # æ³„æ¼æ¦‚ç‡ç™¾åˆ†æ¯”

# æ‰“å°é¢„æµ‹ç»“æœ
for i in range(len(samples)):
  status = "ğŸš¨ æ³„æ¼è­¦æŠ¥ï¼" if predictions[i] == 1 else "âœ… çŠ¶æ€æ­£å¸¸"
  print(f"æ ·æœ¬ {i+1}: æ¸©åº¦={samples[i,0]}â„ƒ | å‹åŠ›={samples[i,1]}MPa | æµé‡={samples[i,2]}mÂ³/s")
  print(f"  é¢„æµ‹ç»“æœ: {status} | æ³„æ¼æ¦‚ç‡: {leak_probabilities[i]:.1f}%")
  print("â”€" * 60)